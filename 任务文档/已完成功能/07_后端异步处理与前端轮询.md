# 07_后端异步处理与前端轮询.md

## 1. 背景与目标
为了解决在长文本生成过程中前端 HTTP 连接超时的问题，我们将 AI 响应的生成过程从实时流式传输（Streaming）改为后端异步处理（Background Task）配合前端轮询（Polling）的机制。

## 2. 核心变更

### 2.1 后端架构调整
- **异步处理**: `/api/chat/route.ts` 接收到请求后，不再保持连接等待 AI 生成结束。
- **立即响应**: API 立即返回 `202 Accepted` 状态码和 `{ status: 'processing' }`，释放 HTTP 连接。
- **后台任务**: 使用 IIFE (Immediately Invoked Function Expression) 在后台触发 `generateText`，将生成的完整回复写入数据库。

### 2.2 前端交互逻辑
- **移除流式 Hook**: 移除了 `useChat` hook 的流式提交功能。
- **乐观更新**: 用户发送消息后，立即在其界面上乐观地添加一条用户消息。
- **轮询机制**:
    - 当状态为 `processing` 时，前端启动定时器（每 2 秒）。
    - 调用 Server Action `getMessagesByProjectId` 获取最新消息列表。
    - 对比本地消息数量，若发现新消息（且为 assistant 角色），则视为生成完成。
    - 更新本地状态并停止轮询。

## 3. 实现细节

### API 路由 (`src/app/api/chat/route.ts`)
```typescript
// 触发后台生成，不使用 await 等待结果
(async () => {
  try {
    const result = await generateText({ ... });
    await saveMessage(projectId, 'assistant', result.text);
  } catch (err) {
    console.error('Background Error:', err);
  }
})();

// 立即返回
return new Response(JSON.stringify({ status: 'processing', projectId }), { status: 202 });
```

### 客户端组件 (`src/app/chat/[id]/chat-page-client.tsx`)
```typescript
// 轮询逻辑
useEffect(() => {
    if (streamingStatus === 'processing') {
        const intervalId = setInterval(async () => {
            const serverMessages = await getMessagesByProjectId(projectId);
            if (serverMessages.length > messages.length) {
                // 更新消息并结束轮询
                setMessages(serverMessages);
                setStreamingStatus('ready');
            }
        }, 2000);
        return () => clearInterval(intervalId);
    }
}, [streamingStatus]);
```

## 4. 优势
- **稳定性**: 彻底避免了 Vercel 或其他 Serverless 环境下的 10s/60s 超时限制。
- **持久化**: 确保每一条 AI 生成的回复都被完整保存到数据库，不会因网络中断而丢失。
- **用户体验**: 虽然失去了“打字机”效果的实时感，但保证了最终结果的可靠交付，且通过“AI 正在思考...”的状态提示保持了用户预期。
